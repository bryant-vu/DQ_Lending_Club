{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project explores memory footprint reduction methods when processing a large dataset; assume working space is limited to 10MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryant\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.95911693572998"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans = pd.read_csv('loans_2007.csv')\n",
    "footprint = loans.memory_usage(deep=True).sum()/(1024*1024)\n",
    "footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading the entire dataset requires 67MB of memory. We can process the dataset in chunks of 5MB or less, just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6168975830078125\n",
      "4.6138458251953125\n",
      "4.6153364181518555\n",
      "4.6162614822387695\n",
      "4.612587928771973\n",
      "4.614043235778809\n",
      "4.613035202026367\n",
      "4.615083694458008\n",
      "4.613102912902832\n",
      "4.612939834594727\n",
      "4.625545501708984\n",
      "4.6223297119140625\n",
      "4.62916374206543\n",
      "4.863524436950684\n",
      "0.8746747970581055\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "for chunk in chunk_iter:\n",
    "    print(chunk.memory_usage(deep=True).sum()/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Each chunk of data is less than 5MB when we process 3000 rows at a time. Let's explore the dataset and find more reduction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     21\n",
      "int64       1\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     22\n",
      "dtype: int64\n",
      "float64    30\n",
      "object     22\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# column datatypes per chunk\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "for chunk in chunk_iter:\n",
    "    print(chunk.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Python, the 'object' datatype consumes more space due to the nature of its design. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's explore the columns in each chunk and see if they are consistent before we continue reducing the memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall obj cols: ['term', 'int_rate', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'revol_util', 'initial_list_status', 'last_pymnt_d', 'last_credit_pull_d', 'application_type'] \n",
      "\n",
      "chunk obj cols: ['id', 'term', 'int_rate', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'revol_util', 'initial_list_status', 'last_pymnt_d', 'last_credit_pull_d', 'application_type'] \n",
      "\n",
      "overall obj cols: ['term', 'int_rate', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'revol_util', 'initial_list_status', 'last_pymnt_d', 'last_credit_pull_d', 'application_type'] \n",
      "\n",
      "chunk obj cols: ['id', 'term', 'int_rate', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'home_ownership', 'verification_status', 'issue_d', 'loan_status', 'pymnt_plan', 'purpose', 'title', 'zip_code', 'addr_state', 'earliest_cr_line', 'revol_util', 'initial_list_status', 'last_pymnt_d', 'last_credit_pull_d', 'application_type'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify unique column headers\n",
    "obj_cols = []\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    chunk_obj_cols = chunk.select_dtypes(include=['object']).columns.tolist()\n",
    "    if len(obj_cols) > 0:\n",
    "        is_same = obj_cols == chunk_obj_cols\n",
    "        if not is_same:\n",
    "            print(\"overall obj cols:\", obj_cols, \"\\n\")\n",
    "            print(\"chunk obj cols:\", chunk_obj_cols, \"\\n\")    \n",
    "    else:\n",
    "        obj_cols = chunk_obj_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There's not much difference in column headers, except that some chunks contain an 'id' column and others do not. Since this is a unique identifier assigned by Lending Club, we can ignore it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's find the percentage of unique values in each column. This will help us understand a bit more about the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary (key: column, value: list of series objects representing each chunk's value counts)\n",
    "chunk_iter = pd.read_csv('loans_2007.csv',chunksize=3000)\n",
    "str_cols_vc = {}\n",
    "for chunk in chunk_iter:\n",
    "    str_cols = chunk.select_dtypes(include=['object'])\n",
    "    for col in str_cols.columns:\n",
    "        current_col_vc = str_cols[col].value_counts()\n",
    "        if col in str_cols_vc:\n",
    "            str_cols_vc[col].append(current_col_vc)\n",
    "        else:\n",
    "            str_cols_vc[col] = [current_col_vc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue_d\n",
      "0.13\n",
      "emp_length\n",
      "0.03\n",
      "verification_status\n",
      "0.01\n",
      "last_pymnt_d\n",
      "0.24\n",
      "title\n",
      "50.01\n",
      "sub_grade\n",
      "0.08\n",
      "term\n",
      "0.0\n",
      "earliest_cr_line\n",
      "1.25\n",
      "home_ownership\n",
      "0.01\n",
      "revol_util\n",
      "2.64\n",
      "pymnt_plan\n",
      "0.0\n",
      "addr_state\n",
      "0.12\n",
      "last_credit_pull_d\n",
      "0.25\n",
      "emp_title\n",
      "76.82\n",
      "loan_status\n",
      "0.02\n",
      "purpose\n",
      "0.03\n",
      "id\n",
      "100.0\n",
      "grade\n",
      "0.02\n",
      "initial_list_status\n",
      "0.0\n",
      "application_type\n",
      "0.0\n",
      "zip_code\n",
      "1.97\n",
      "int_rate\n",
      "0.93\n"
     ]
    }
   ],
   "source": [
    "# combine the value counts an to get an idea of how many unique values are in each column\n",
    "combined_vcs = {}\n",
    "\n",
    "for col in str_cols_vc:\n",
    "    combined_vc = pd.concat(str_cols_vc[col])\n",
    "    final_vc = combined_vc.groupby(combined_vc.index).sum()\n",
    "    combined_vcs[col] = final_vc\n",
    "    \n",
    "for key in combined_vcs:\n",
    "    unique = len(combined_vcs[key])\n",
    "    total = combined_vcs[key].sum()\n",
    "    percent = round(unique/total * 100,2)\n",
    "    print(key)\n",
    "    print(percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Makes sense. Columns like 'verification_status' and 'application_type' will not have wide ranges of values as opposed to 'id' and 'title' which are both made up of over 50% unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's now look at the list of column headers and identify a few object types that can be converted to a different data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term',\n",
       " 'int_rate',\n",
       " 'grade',\n",
       " 'sub_grade',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'verification_status',\n",
       " 'issue_d',\n",
       " 'loan_status',\n",
       " 'pymnt_plan',\n",
       " 'purpose',\n",
       " 'title',\n",
       " 'zip_code',\n",
       " 'addr_state',\n",
       " 'earliest_cr_line',\n",
       " 'revol_util',\n",
       " 'initial_list_status',\n",
       " 'last_pymnt_d',\n",
       " 'last_credit_pull_d',\n",
       " 'application_type']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_obj_cols = ['term', 'sub_grade', 'emp_title', 'home_ownership', 'verification_status', 'issue_d', 'purpose', 'earliest_cr_line', 'revol_util', 'last_pymnt_d', 'last_credit_pull_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term\n",
      " 36 months    31534\n",
      " 60 months    11001\n",
      "Name: term, dtype: int64\n",
      "-----------\n",
      "sub_grade\n",
      "A1    1142\n",
      "A2    1520\n",
      "A3    1823\n",
      "A4    2905\n",
      "A5    2793\n",
      "B1    1882\n",
      "B2    2113\n",
      "B3    2997\n",
      "B4    2590\n",
      "B5    2807\n",
      "C1    2264\n",
      "C2    2157\n",
      "C3    1658\n",
      "C4    1370\n",
      "C5    1291\n",
      "D1    1053\n",
      "D2    1485\n",
      "D3    1322\n",
      "D4    1140\n",
      "D5    1016\n",
      "E1     884\n",
      "E2     791\n",
      "E3     668\n",
      "E4     552\n",
      "E5     499\n",
      "F1     392\n",
      "F2     308\n",
      "F3     236\n",
      "F4     211\n",
      "F5     154\n",
      "G1     141\n",
      "G2     107\n",
      "G3      79\n",
      "G4      99\n",
      "G5      86\n",
      "Name: sub_grade, dtype: int64\n",
      "-----------\n",
      "emp_title\n",
      "  old palm inc                                               1\n",
      " Brocade Communications                                      1\n",
      " CenturyLink                                                 1\n",
      " Department of Homeland Security                             1\n",
      " Down To Earth Distributors, Inc.                            1\n",
      " Plaid, Inc.                                                 1\n",
      " U.S. Dept. Of Homeland Security                             1\n",
      " pacific crane maintenance company                           1\n",
      "$260M '06 vintage technology venture capital firm            1\n",
      "(Collaborative) Abbott Nutrition Intl                        1\n",
      "(self) Castleforte Group                                     1\n",
      "0CEAN VIEW INTL REALTY/C21 UNITED PLATINUM                   1\n",
      "1 Cochran Hyundai                                            1\n",
      "1 and 1 Internet Inc.                                        1\n",
      "1)-Yavapai Regional Medical Center 2)- Dr. cantors office    1\n",
      "1-800 Contacts                                               1\n",
      "101 restaurant                                               1\n",
      "1013 communications                                          1\n",
      "1125 memorex dr                                              1\n",
      "1199SEIU Benefit and Pension Funds                           1\n",
      "1199SEIU UHE                                                 1\n",
      "123 Recovery                                                 1\n",
      "123 appartment corp.                                         1\n",
      "128 Air Refueling Wing (USAF)                                1\n",
      "1400                                                         1\n",
      "15                                                           1\n",
      "162 fighter wing                                             1\n",
      "16th MP BDE, U.S. Army                                       1\n",
      "1800-got-junk?                                               1\n",
      "182nd Airlift Wing Fire Dept.                                1\n",
      "                                                            ..\n",
      "wyoming valley hospital                                      1\n",
      "wythe county community hospital                              1\n",
      "xcel fire protection                                         1\n",
      "xerox cor.                                                   1\n",
      "xerox corp                                                   1\n",
      "xo communication                                             1\n",
      "xotic deliveries                                             1\n",
      "xpedite technologies inc                                     1\n",
      "xpedx                                                        1\n",
      "yankee candle company                                        1\n",
      "ye olde pepper co                                            1\n",
      "yellow cab com.                                              1\n",
      "yellow cab taxi                                              1\n",
      "ymca                                                         3\n",
      "yonkers fire department                                      1\n",
      "york hospital dental center                                  1\n",
      "yorkville physical therapy                                   1\n",
      "young and associates                                         1\n",
      "young atitudes aveda                                         1\n",
      "youth villages                                               1\n",
      "yrbnyc.com / Yellow Rat Bastard                              1\n",
      "yrc                                                          1\n",
      "yrcw                                                         1\n",
      "zachry industral inc                                         1\n",
      "zakheim and lavrar                                           1\n",
      "zashko inc.                                                  1\n",
      "zeno office solutions                                        1\n",
      "zion lutheran school                                         1\n",
      "zoll medical corp                                            1\n",
      "zozaya officiating                                           1\n",
      "Name: emp_title, dtype: int64\n",
      "-----------\n",
      "home_ownership\n",
      "MORTGAGE    18959\n",
      "NONE            8\n",
      "OTHER         136\n",
      "OWN          3251\n",
      "RENT        20181\n",
      "Name: home_ownership, dtype: int64\n",
      "-----------\n",
      "verification_status\n",
      "Not Verified       18758\n",
      "Source Verified    10306\n",
      "Verified           13471\n",
      "Name: verification_status, dtype: int64\n",
      "-----------\n",
      "issue_d\n",
      "Apr-2008     259\n",
      "Apr-2009     333\n",
      "Apr-2010     912\n",
      "Apr-2011    1563\n",
      "Aug-2007      74\n",
      "Aug-2008     100\n",
      "Aug-2009     446\n",
      "Aug-2010    1175\n",
      "Aug-2011    1934\n",
      "Dec-2007     172\n",
      "Dec-2008     253\n",
      "Dec-2009     658\n",
      "Dec-2010    1335\n",
      "Dec-2011    2267\n",
      "Feb-2008     306\n",
      "Feb-2009     302\n",
      "Feb-2010     682\n",
      "Feb-2011    1298\n",
      "Jan-2008     305\n",
      "Jan-2009     269\n",
      "Jan-2010     662\n",
      "Jan-2011    1380\n",
      "Jul-2007      63\n",
      "Jul-2008     141\n",
      "Jul-2009     411\n",
      "Jul-2010    1204\n",
      "Jul-2011    1875\n",
      "Jun-2007      24\n",
      "Jun-2008     124\n",
      "Jun-2009     406\n",
      "Jun-2010    1105\n",
      "Jun-2011    1835\n",
      "Mar-2008     402\n",
      "Mar-2009     324\n",
      "Mar-2010     828\n",
      "Mar-2011    1448\n",
      "May-2008     115\n",
      "May-2009     359\n",
      "May-2010     989\n",
      "May-2011    1704\n",
      "Nov-2007     112\n",
      "Nov-2008     209\n",
      "Nov-2009     662\n",
      "Nov-2010    1224\n",
      "Nov-2011    2232\n",
      "Oct-2007     105\n",
      "Oct-2008     122\n",
      "Oct-2009     604\n",
      "Oct-2010    1232\n",
      "Oct-2011    2118\n",
      "Sep-2007      53\n",
      "Sep-2008      57\n",
      "Sep-2009     507\n",
      "Sep-2010    1189\n",
      "Sep-2011    2067\n",
      "Name: issue_d, dtype: int64\n",
      "-----------\n",
      "purpose\n",
      "car                    1615\n",
      "credit_card            5477\n",
      "debt_consolidation    19776\n",
      "educational             422\n",
      "home_improvement       3199\n",
      "house                   426\n",
      "major_purchase         2311\n",
      "medical                 753\n",
      "moving                  629\n",
      "other                  4425\n",
      "renewable_energy        106\n",
      "small_business         1992\n",
      "vacation                400\n",
      "wedding                1004\n",
      "Name: purpose, dtype: int64\n",
      "-----------\n",
      "earliest_cr_line\n",
      "Apr-1964      3\n",
      "Apr-1966      1\n",
      "Apr-1967      4\n",
      "Apr-1968      1\n",
      "Apr-1969      1\n",
      "Apr-1970      3\n",
      "Apr-1971      6\n",
      "Apr-1972      2\n",
      "Apr-1973      7\n",
      "Apr-1974      6\n",
      "Apr-1975     11\n",
      "Apr-1976      7\n",
      "Apr-1977     10\n",
      "Apr-1978     14\n",
      "Apr-1979     15\n",
      "Apr-1980     17\n",
      "Apr-1981     15\n",
      "Apr-1982     23\n",
      "Apr-1983     29\n",
      "Apr-1984     27\n",
      "Apr-1985     31\n",
      "Apr-1986     29\n",
      "Apr-1987     48\n",
      "Apr-1988     55\n",
      "Apr-1989     64\n",
      "Apr-1990     79\n",
      "Apr-1991     77\n",
      "Apr-1992     90\n",
      "Apr-1993    100\n",
      "Apr-1994    134\n",
      "           ... \n",
      "Sep-1979     20\n",
      "Sep-1980     18\n",
      "Sep-1981     28\n",
      "Sep-1982     16\n",
      "Sep-1983     33\n",
      "Sep-1984     46\n",
      "Sep-1985     41\n",
      "Sep-1986     59\n",
      "Sep-1987     59\n",
      "Sep-1988     63\n",
      "Sep-1989     83\n",
      "Sep-1990     93\n",
      "Sep-1991     70\n",
      "Sep-1992     72\n",
      "Sep-1993    124\n",
      "Sep-1994    119\n",
      "Sep-1995    181\n",
      "Sep-1996    187\n",
      "Sep-1997    196\n",
      "Sep-1998    267\n",
      "Sep-1999    277\n",
      "Sep-2000    325\n",
      "Sep-2001    282\n",
      "Sep-2002    251\n",
      "Sep-2003    220\n",
      "Sep-2004    221\n",
      "Sep-2005    162\n",
      "Sep-2006    150\n",
      "Sep-2007     63\n",
      "Sep-2008      8\n",
      "Name: earliest_cr_line, dtype: int64\n",
      "-----------\n",
      "revol_util\n",
      "0%       1070\n",
      "0.01%       1\n",
      "0.03%       1\n",
      "0.04%       1\n",
      "0.05%       1\n",
      "0.1%       61\n",
      "0.12%       1\n",
      "0.16%       1\n",
      "0.2%       64\n",
      "0.3%       43\n",
      "0.4%       45\n",
      "0.46%       1\n",
      "0.49%       1\n",
      "0.5%       49\n",
      "0.54%       1\n",
      "0.6%       39\n",
      "0.7%       47\n",
      "0.75%       1\n",
      "0.8%       43\n",
      "0.83%       1\n",
      "0.86%       1\n",
      "0.9%       41\n",
      "1%         51\n",
      "1.1%       39\n",
      "1.2%       37\n",
      "1.3%       36\n",
      "1.4%       32\n",
      "1.5%       33\n",
      "1.6%       24\n",
      "1.7%       30\n",
      "         ... \n",
      "97%        31\n",
      "97.1%      39\n",
      "97.2%      32\n",
      "97.3%      36\n",
      "97.4%      33\n",
      "97.5%      35\n",
      "97.6%      35\n",
      "97.7%      45\n",
      "97.8%      38\n",
      "97.9%      27\n",
      "98%        28\n",
      "98.1%      25\n",
      "98.2%      25\n",
      "98.3%      36\n",
      "98.4%      31\n",
      "98.5%      29\n",
      "98.6%      36\n",
      "98.7%      27\n",
      "98.8%      38\n",
      "98.9%      20\n",
      "99%        35\n",
      "99.1%      33\n",
      "99.2%      22\n",
      "99.3%      32\n",
      "99.4%      24\n",
      "99.5%      24\n",
      "99.6%      27\n",
      "99.7%      32\n",
      "99.8%      25\n",
      "99.9%      29\n",
      "Name: revol_util, dtype: int64\n",
      "-----------\n",
      "last_pymnt_d\n",
      "Apr-2008     23\n",
      "Apr-2009     72\n",
      "Apr-2010    145\n",
      "Apr-2011    519\n",
      "Apr-2012    781\n",
      "Apr-2013    890\n",
      "Apr-2014    682\n",
      "Apr-2015    141\n",
      "Apr-2016    178\n",
      "Aug-2008     31\n",
      "Aug-2009     66\n",
      "Aug-2010    198\n",
      "Aug-2011    479\n",
      "Aug-2012    868\n",
      "Aug-2013    744\n",
      "Aug-2014    836\n",
      "Aug-2015    217\n",
      "Dec-2007      2\n",
      "Dec-2008     31\n",
      "Dec-2009    116\n",
      "Dec-2010    332\n",
      "Dec-2011    585\n",
      "Dec-2012    732\n",
      "Dec-2013    799\n",
      "Dec-2014    949\n",
      "Dec-2015    185\n",
      "Feb-2008      8\n",
      "Feb-2009     65\n",
      "Feb-2010    148\n",
      "Feb-2011    437\n",
      "           ... \n",
      "May-2011    448\n",
      "May-2012    773\n",
      "May-2013    943\n",
      "May-2014    686\n",
      "May-2015    151\n",
      "May-2016    203\n",
      "Nov-2008     31\n",
      "Nov-2009     80\n",
      "Nov-2010    258\n",
      "Nov-2011    487\n",
      "Nov-2012    782\n",
      "Nov-2013    701\n",
      "Nov-2014    593\n",
      "Nov-2015    231\n",
      "Oct-2008     55\n",
      "Oct-2009     85\n",
      "Oct-2010    292\n",
      "Oct-2011    482\n",
      "Oct-2012    853\n",
      "Oct-2013    712\n",
      "Oct-2014    813\n",
      "Oct-2015    200\n",
      "Sep-2008     40\n",
      "Sep-2009     55\n",
      "Sep-2010    215\n",
      "Sep-2011    491\n",
      "Sep-2012    802\n",
      "Sep-2013    712\n",
      "Sep-2014    694\n",
      "Sep-2015    211\n",
      "Name: last_pymnt_d, dtype: int64\n",
      "-----------\n",
      "last_credit_pull_d\n",
      "Apr-2009     24\n",
      "Apr-2010     77\n",
      "Apr-2011    177\n",
      "Apr-2012    326\n",
      "Apr-2013    445\n",
      "Apr-2014    507\n",
      "Apr-2015    404\n",
      "Apr-2016    912\n",
      "Aug-2007     17\n",
      "Aug-2008     15\n",
      "Aug-2009     32\n",
      "Aug-2010     68\n",
      "Aug-2011    290\n",
      "Aug-2012    391\n",
      "Aug-2013    375\n",
      "Aug-2014    528\n",
      "Aug-2015    435\n",
      "Dec-2007      3\n",
      "Dec-2008      7\n",
      "Dec-2009     73\n",
      "Dec-2010    156\n",
      "Dec-2011    268\n",
      "Dec-2012    391\n",
      "Dec-2013    431\n",
      "Dec-2014    567\n",
      "Dec-2015    656\n",
      "Feb-2008      3\n",
      "Feb-2009     20\n",
      "Feb-2010     87\n",
      "Feb-2011    199\n",
      "           ... \n",
      "May-2013    470\n",
      "May-2014    477\n",
      "May-2015    433\n",
      "May-2016    766\n",
      "Nov-2007      3\n",
      "Nov-2009     50\n",
      "Nov-2010    126\n",
      "Nov-2011    221\n",
      "Nov-2012    432\n",
      "Nov-2013    491\n",
      "Nov-2014    500\n",
      "Nov-2015    516\n",
      "Oct-2007      3\n",
      "Oct-2008     13\n",
      "Oct-2009     51\n",
      "Oct-2010    120\n",
      "Oct-2011    162\n",
      "Oct-2012    351\n",
      "Oct-2013    429\n",
      "Oct-2014    529\n",
      "Oct-2015    513\n",
      "Sep-2007      5\n",
      "Sep-2008      9\n",
      "Sep-2009     21\n",
      "Sep-2010    119\n",
      "Sep-2011    175\n",
      "Sep-2012    414\n",
      "Sep-2013    408\n",
      "Sep-2014    564\n",
      "Sep-2015    531\n",
      "Name: last_credit_pull_d, dtype: int64\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for col in useful_obj_cols:\n",
    "    print(col)\n",
    "    print(combined_vcs[col])\n",
    "    print(\"-----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following columns can be converted to categorical types since there is minimal variation within each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_col_dtypes = {\n",
    "    \"sub_grade\": \"category\", \"home_ownership\": \"category\", \n",
    "    \"verification_status\": \"category\", \"purpose\": \"category\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Additionally, columns 'issue_d', 'earliest_cr_line', 'last_pymnt_d', 'last_credit_pull_d' can be converted to datetime type while 'term' and 'revol_util' can be converted to numeric types, after a bit of cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.909579277038574\n",
      "2.906174659729004\n",
      "2.908400535583496\n",
      "2.9086151123046875\n",
      "2.905801773071289\n",
      "2.9066877365112305\n",
      "2.9060230255126953\n",
      "2.907994270324707\n",
      "2.906473159790039\n",
      "2.907724380493164\n",
      "2.920522689819336\n",
      "2.9192609786987305\n",
      "2.9269638061523438\n",
      "3.159947395324707\n",
      "0.5741252899169922\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000, dtype=convert_col_dtypes, parse_dates=[\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\"])\n",
    "for chunk in chunk_iter:\n",
    "    term_cleaned = chunk['term'].str.lstrip(\" \").str.rstrip(\" months\")\n",
    "    revol_cleaned = chunk['revol_util'].str.rstrip(\"%\")\n",
    "    chunk['term'] = pd.to_numeric(term_cleaned)\n",
    "    chunk['revol_util'] = pd.to_numeric(revol_cleaned)\n",
    "    print(chunk.memory_usage(deep=True).sum()/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We see a significant reduction in memory footprint already. For the final touch, we will eliminate rows where values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.770437240600586\n",
      "2.7043275833129883\n",
      "2.7438554763793945\n",
      "2.748079299926758\n",
      "2.7244319915771484\n",
      "2.7583799362182617\n",
      "2.720165252685547\n",
      "2.760761260986328\n",
      "2.766282081604004\n",
      "2.7846755981445312\n",
      "2.7481870651245117\n",
      "2.7025365829467773\n",
      "2.826213836669922\n",
      "2.150092124938965\n",
      "0.027849197387695312\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000, dtype=convert_col_dtypes, parse_dates=[\"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\"])\n",
    "\n",
    "tot = 0\n",
    "for chunk in chunk_iter:\n",
    "    term_cleaned = chunk['term'].str.lstrip(\" \").str.rstrip(\" months\")\n",
    "    revol_cleaned = chunk['revol_util'].str.rstrip(\"%\")\n",
    "    chunk['term'] = pd.to_numeric(term_cleaned)\n",
    "    chunk['revol_util'] = pd.to_numeric(revol_cleaned)\n",
    "    chunk = chunk.dropna(how='any')\n",
    "    tot += chunk.memory_usage(deep=True).sum()/(1024*1024)\n",
    "    print(chunk.memory_usage(deep=True).sum()/(1024*1024))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Let's compare this reduced footprint with the initial ~67MB footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.34\n"
     ]
    }
   ],
   "source": [
    "reduction = (footprint - tot) / footprint\n",
    "print(round(reduction*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We achieved a 43% reduction in memory footprint by converting 10 columns of object data type during the initial processing of the csv file! By processing the data in chunks, we can bypass storage limitations with relative ease."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
